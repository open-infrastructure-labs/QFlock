for INDEX in {1000..1023}; do ./docker-bench.py --query_range $INDEX --no_catalog --log_level TRACE > log$INDEX.txt ; echo "$INDEX $(./parse_log.py log$INDEX.txt)"; done

for INDEX in {1000..1023}; do echo "$INDEX $(grep queries/tpcds/tmp log$INDEX.txt| cut -d' ' -f9)"; done

tcpdump -r ./log_tcpdump3.td > log_tcpdump3.txt

sudo tcpdump -n -s 128  src host 172.18.0.2 -w ./log_tcpdump3.td

sudo tc qdisc del dev eth0 root
sudo tc qdisc add dev eth0 root tbf rate 128kbit limit 64kb burst 64kb

./docker-bench.py --query_range 1000-1022 --ext --explain
./docker-bench.py --queries 1000-1022 --capture_log_level TRACE --no_catalog --terse

pyspark --master local --conf "spark.driver.memory=2g" --conf "spark.driver.maxResultSize=2g" --conf "spark.executor.memory=2g"
df = spark.read.option("url", "jdbc:hive2://localhost:10001/tpcds;transportMode=http;httpPath=cliservice").format("jdbc").option("dbtable", "store_sales").load()

!connect jdbc:hive2://localhost:10001/;transportMode=http;httpPath=cliservice -n rob

from py4j.java_gateway import java_import
gw = spark.sparkContext._gateway
java_import(gw.jvm, "com.github.qflock.extensions.QflockJdbcDialect")
gw.jvm.org.apache.spark.sql.jdbc.JdbcDialects.registerDialect(gw.jvm.com.github.qflock.extensions.QflockJdbcDialect())

beeline -u 'jdbc:hive2://localhost:10001/;transportMode=http;httpPath=cliservice' -n rob --outputformat=csv2 -e "select * from tpcds.call_center;" > /tmp/output.csv

import pyarrow
import pyarrow.parquet
reader = pyarrow.parquet.ParquetFile("/data/store_sales.parquet/part-00000-2b94e42c-0762-4085-ad6e-b3f057eca1d8-c000.snappy.parquet")
rg = reader.metadata.row_group(0)
col_sizes = [0] * rg.num_columns
for rgi in range(0, reader.num_row_groups):
    rg = reader.metadata.row_group(rgi)
    for col_idx in range(0, rg.num_columns):
        col_info = rg.column(col_idx)
        col_sizes[col_idx] += col_info.total_compressed_size
        print(f"{rgi}:{col_idx} = {col_sizes[col_idx]}")