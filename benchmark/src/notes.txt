for INDEX in {1000..1023}; do ./docker-bench.py --query_range $INDEX --no_catalog --log_level TRACE > log$INDEX.txt ; echo "$INDEX $(./parse_log.py log$INDEX.txt)"; done

for INDEX in {1000..1023}; do echo "$INDEX $(grep queries/tpcds/tmp log$INDEX.txt| cut -d' ' -f9)"; done

tcpdump -r ./log_tcpdump3.td > log_tcpdump3.txt

sudo tcpdump -n -s 128  src host 172.18.0.2 -w ./log_tcpdump3.td

sudo tc qdisc del dev eth0 root
sudo tc qdisc add dev eth0 root tbf rate 128kbit limit 64kb burst 64kb

./docker-bench.py --query_range 1000-1022 --ext --explain
./docker-bench.py --queries 1000-1022 --capture_log_level TRACE --no_catalog --terse

pyspark --master local --conf "spark.driver.memory=2g" --conf "spark.driver.maxResultSize=2g" --conf "spark.executor.memory=2g"
df = spark.read.option("url", "jdbc:hive2://localhost:10001/tpcds;transportMode=http;httpPath=cliservice").format("jdbc").option("dbtable", "store_sales").load()

!connect jdbc:hive2://localhost:10001/;transportMode=http;httpPath=cliservice -n rob

from py4j.java_gateway import java_import
gw = spark.sparkContext._gateway
java_import(gw.jvm, "com.github.qflock.extensions.QflockJdbcDialect")
gw.jvm.org.apache.spark.sql.jdbc.JdbcDialects.registerDialect(gw.jvm.com.github.qflock.extensions.QflockJdbcDialect())

beeline -u 'jdbc:hive2://localhost:10001/;transportMode=http;httpPath=cliservice' -n rob --outputformat=csv2 -e "select * from tpcds.call_center;" > /tmp/output.csv

import pyarrow
import pyarrow.parquet
reader = pyarrow.parquet.ParquetFile("/data/store_sales.parquet/part-00000-2b94e42c-0762-4085-ad6e-b3f057eca1d8-c000.snappy.parquet")
rg = reader.metadata.row_group(0)
col_sizes = [0] * rg.num_columns
for rgi in range(0, reader.num_row_groups):
    rg = reader.metadata.row_group(rgi)
    for col_idx in range(0, rg.num_columns):
        col_info = rg.column(col_idx)
        col_sizes[col_idx] += col_info.total_compressed_size
        print(f"{rgi}:{col_idx} = {col_sizes[col_idx]}")

HADOOP_ROOT_LOGGER=hadoop.root.logger=DEBUG,console

# Join
SELECT ss_sold_date_sk,ss_item_sk,ss_store_sk,ss_quantity,sr_returned_date_sk,sr_item_sk,sr_customer_sk,sr_return_quantity FROM ( (SELECT ss_sold_date_sk,ss_item_sk,ss_customer_sk,ss_store_sk,ss_ticket_number,ss_quantity FROM store_sales_0 WHERE ss_customer_sk IS NOT NULL AND ss_item_sk IS NOT NULL AND ss_ticket_number IS NOT NULL AND ss_sold_date_sk IS NOT NULL AND ss_store_sk IS NOT NULL ) a INNER JOIN (SELECT sr_returned_date_sk,sr_item_sk,sr_customer_sk,sr_ticket_number,sr_return_quantity FROM store_returns WHERE sr_customer_sk IS NOT NULL AND sr_item_sk IS NOT NULL AND sr_ticket_number IS NOT NULL AND sr_returned_date_sk IS NOT NULL ) b ON (a.ss_customer_sk = b.sr_customer_sk AND a.ss_item_sk = b.sr_item_sk AND a.ss_ticket_number = b.sr_ticket_number))
SELECT ss_sold_date_sk FROM ( (SELECT * FROM store_sales) a INNER JOIN (SELECT * FROM store_returns  b ON (a.ss_customer_sk = b.sr_customer_sk AND a.ss_item_sk = b.sr_item_sk AND a.ss_ticket_number = b.sr_ticket_number))
# String query

"select i_item_id,i_rec_start_date,i_rec_end_date,i_item_desc,i_brand,i_class,i_category,i_manufact,i_size,i_formulation,i_color,i_units,i_container,i_product_name from ((select * from store_sales) inner join  (select * from item) ON (ss_item_sk = i_item_sk))"
"select c_customer_id,c_salutation,c_first_name,c_last_name,c_preferred_cust_flag,c_birth_country,c_login,c_email_address,c_last_review_date from customer"
"select i_item_id,i_rec_start_date,i_rec_end_date,i_desc,i_brand,i_class,i_category,i_manufact,i_size,i_formulation,i_color,i_units,i_container,i_product_name from item"